# -*- coding: utf-8 -*-
"""ML 3 Days.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jRMAxOGnIAtoPoPK1SXUXLyrbctxxSqw
"""

# import pandas as pd
# import matplotlib.pyplot as plt
# import seaborn as sns
# from sklearn.model_selection import train_test_split, cross_val_score
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.preprocessing import StandardScaler
# from sklearn.metrics import accuracy_score, classification_report

# # Step 1: Load the dataset
# df = pd.read_csv('Iris.csv')

# # Step 2: Preprocess the data
# df = df.drop(columns=['Id'])  # Remove ID column
# print("First 20 rows:\n", df.head(20))
# print("Shape of dataset:", df.shape)
# print("Missing values:\n", df.isnull().sum())
# print("Class distribution:\n", df['Species'].value_counts())

# # Step 3: Visualize the data
# sns.pairplot(df, hue="Species")
# plt.suptitle("Iris Feature Pairplots", y=1.02)
# plt.savefig('iris_pairplot.png')
# plt.show()

# # Step 4: Feature and target selection
# X = df.drop(columns=['Species'])  # Features
# y = df['Species']  # Target

# # Optional: Scaling (not required for Random Forest, but good practice)
# scaler = StandardScaler()
# X_scaled = scaler.fit_transform(X)

# # Step 5: Train/Test Split
# X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# # Step 6: Train the model
# model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)
# model.fit(X_train, y_train)

# # Step 7: Evaluate the model
# y_pred = model.predict(X_test)
# accuracy = accuracy_score(y_test, y_pred)
# print(f"\nTest Accuracy: {accuracy:.2f}")
# print("Classification Report:\n", classification_report(y_test, y_pred))

# # Step 8: Cross-validation
# cv_scores = cross_val_score(model, X_scaled, y, cv=5)
# print("Cross-validation scores:", cv_scores)
# print("Mean CV Accuracy:", cv_scores.mean())

from pandas import read_csv
from matplotlib import pyplot
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
import joblib

# Step 1: Load dataset
filename = "Iris.csv"
data = read_csv(filename)

# Step 2: Display data shape and preview
print("Shape of the dataset:", data.shape)
print("First 20 rows:\n", data.head(20))

# Step 3: Plot and save histograms silently
data.hist()
pyplot.savefig("histograms.png")
pyplot.close()  # Close the plot so it doesn't show up in prompt

# Step 4: Plot and save density plots silently
data.plot(kind='density', subplots=True, layout=(3,3), sharex=False)
pyplot.savefig("density_plots.png")
pyplot.close()

# Step 5: Convert to NumPy array and extract features/labels
array = data.values
X = array[:, 1:5]  # Features: Sepal/Petal measurements
Y = array[:, 5]    # Target: Species

# Step 6: Split data into training (67%) and testing (33%)
test_size = 0.33
seed = 7
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)

# Step 7: Create and train logistic regression model
model = LogisticRegression(max_iter=200)
model.fit(X_train, Y_train)

# Step 8: Evaluate and display accuracy
result = model.score(X_test, Y_test)
print("Accuracy: {:.2f}%".format(result * 100))

# Step 9: Save the trained model to a file
joblib.dump(model, "logistic_model.pkl")